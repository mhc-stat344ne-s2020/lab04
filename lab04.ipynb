{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab04.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssyEy81nY67K",
        "colab_type": "text"
      },
      "source": [
        "We're going to work with one of the examples from Chollet in this lab.  In this example, our goal is to use text movie reviews from IMDB (the Internet Move DataBase) as input, and our goal will be to classify the reviews as either \"positive\" or \"negative\".\n",
        "\n",
        "I have taken all of the code below to preprocess the data and fit a model in Keras straight out of Chapter 4 of Chollet.  The book describes how the code works in some detail, so I encourage you to give it a read.\n",
        "\n",
        "### Import modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjIHVO6aJQg7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "from keras.datasets import imdb\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.special import expit as sigmoid\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfXUwnHKlM2m",
        "colab_type": "text"
      },
      "source": [
        "### Load data and preprocess it\n",
        "This will take a few seconds to run; it downloads a large data set.\n",
        "\n",
        "We print out a \"decoded\" review to see what they look like.  But the reviews themselves are one-hot encoded (see next code cell)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyXiHGfW3e1j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
        "word_index = imdb.get_word_index()\n",
        "reverse_word_index = dict(\n",
        "    [(value, key) for (key, value) in word_index.items()])\n",
        "decoded_review = ' '.join(\n",
        "    [reverse_word_index.get(i - 3, '?') for i in train_data[0]])\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, sequence] = 1.\n",
        "    return results\n",
        "\n",
        "x_train = vectorize_sequences(train_data)\n",
        "x_test = vectorize_sequences(test_data)\n",
        "y_train = np.asarray(train_labels).astype('float32')\n",
        "y_test = np.asarray(test_labels).astype('float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2v3ZYAg6mwJr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "f4dec83a-877d-4ee6-94a9-7a2d504488a0"
      },
      "source": [
        "print(\"x_train shape = \" + str(x_train.shape))\n",
        "print(\"y_train shape = \" + str(y_train.shape))\n",
        "print(\"The first review: \" + decoded_review)\n",
        "print(\"The first review encoded as 1's and 0's for presence/absence of each of the 10000 most commonly occurring words.\")\n",
        "print(x_train[0, :]) # currently, x_train has observations in rows; this prints the "
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape = (25000, 10000)\n",
            "y_train shape = (25000,)\n",
            "The first review: ? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy's that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n",
            "The first review encoded as 1's and 0's for presence/absence of each of the 10000 most commonly occurring words.\n",
            "[0. 1. 1. ... 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVgnF1odm0wj",
        "colab_type": "text"
      },
      "source": [
        "### Define Keras model and fit it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wr1aZ_TomwMs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(16, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cAFmYIDmd13",
        "colab_type": "text"
      },
      "source": [
        "### Split into train and validation sets, fit the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_Oz72ts6Ikt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 737
        },
        "outputId": "0c725d3d-4818-496a-f3c4-c7e881b60dfd"
      },
      "source": [
        "x_val = x_train[:10000]\n",
        "partial_x_train = x_train[10000:]\n",
        "y_val = y_train[:10000]\n",
        "partial_y_train = y_train[10000:]\n",
        "\n",
        "history = model.fit(partial_x_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=20,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(x_val, y_val))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 15000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "15000/15000 [==============================] - 2s 103us/step - loss: 0.0021 - acc: 0.9999 - val_loss: 0.7253 - val_acc: 0.8606\n",
            "Epoch 2/20\n",
            "15000/15000 [==============================] - 1s 99us/step - loss: 0.0041 - acc: 0.9994 - val_loss: 0.7379 - val_acc: 0.8652\n",
            "Epoch 3/20\n",
            "15000/15000 [==============================] - 1s 99us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.7655 - val_acc: 0.8654\n",
            "Epoch 4/20\n",
            "15000/15000 [==============================] - 2s 100us/step - loss: 0.0046 - acc: 0.9989 - val_loss: 0.7861 - val_acc: 0.8647\n",
            "Epoch 5/20\n",
            "15000/15000 [==============================] - 2s 102us/step - loss: 6.8074e-04 - acc: 1.0000 - val_loss: 0.8069 - val_acc: 0.8640\n",
            "Epoch 6/20\n",
            "15000/15000 [==============================] - 1s 100us/step - loss: 6.0321e-04 - acc: 1.0000 - val_loss: 0.8389 - val_acc: 0.8643\n",
            "Epoch 7/20\n",
            "15000/15000 [==============================] - 2s 100us/step - loss: 0.0047 - acc: 0.9988 - val_loss: 0.8770 - val_acc: 0.8648\n",
            "Epoch 8/20\n",
            "15000/15000 [==============================] - 2s 102us/step - loss: 3.0205e-04 - acc: 1.0000 - val_loss: 0.8854 - val_acc: 0.8637\n",
            "Epoch 9/20\n",
            "15000/15000 [==============================] - 2s 103us/step - loss: 2.4989e-04 - acc: 1.0000 - val_loss: 0.9117 - val_acc: 0.8634\n",
            "Epoch 10/20\n",
            "15000/15000 [==============================] - 2s 103us/step - loss: 0.0017 - acc: 0.9995 - val_loss: 0.9988 - val_acc: 0.8516\n",
            "Epoch 11/20\n",
            "15000/15000 [==============================] - 2s 104us/step - loss: 2.5128e-04 - acc: 1.0000 - val_loss: 0.9493 - val_acc: 0.8608\n",
            "Epoch 12/20\n",
            "15000/15000 [==============================] - 2s 103us/step - loss: 1.3695e-04 - acc: 1.0000 - val_loss: 0.9637 - val_acc: 0.8618\n",
            "Epoch 13/20\n",
            "15000/15000 [==============================] - 2s 103us/step - loss: 1.1100e-04 - acc: 1.0000 - val_loss: 0.9921 - val_acc: 0.8602\n",
            "Epoch 14/20\n",
            "15000/15000 [==============================] - 2s 102us/step - loss: 0.0015 - acc: 0.9995 - val_loss: 1.0245 - val_acc: 0.8624\n",
            "Epoch 15/20\n",
            "15000/15000 [==============================] - 2s 105us/step - loss: 7.7897e-05 - acc: 1.0000 - val_loss: 1.0256 - val_acc: 0.8612\n",
            "Epoch 16/20\n",
            "15000/15000 [==============================] - 2s 103us/step - loss: 5.7088e-05 - acc: 1.0000 - val_loss: 1.0372 - val_acc: 0.8615\n",
            "Epoch 17/20\n",
            "15000/15000 [==============================] - 2s 103us/step - loss: 4.6087e-05 - acc: 1.0000 - val_loss: 1.0644 - val_acc: 0.8601\n",
            "Epoch 18/20\n",
            "15000/15000 [==============================] - 2s 103us/step - loss: 0.0033 - acc: 0.9991 - val_loss: 1.1058 - val_acc: 0.8604\n",
            "Epoch 19/20\n",
            "15000/15000 [==============================] - 2s 103us/step - loss: 3.1053e-05 - acc: 1.0000 - val_loss: 1.1011 - val_acc: 0.8620\n",
            "Epoch 20/20\n",
            "15000/15000 [==============================] - 2s 100us/step - loss: 2.3970e-05 - acc: 1.0000 - val_loss: 1.1075 - val_acc: 0.8623\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpb6Iw9GzIP1",
        "colab_type": "text"
      },
      "source": [
        "### Plot training and validation set loss and accuracy over the course of model estimation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_KQmqgw7Mbl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "16face9d-a152-4d6d-943d-9113b1b67af6"
      },
      "source": [
        "history_dict = history.history\n",
        "history_dict.keys()\n",
        "loss_values = history_dict['loss']\n",
        "val_loss_values = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(loss_values) + 1)\n",
        "\n",
        "plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkZhTIhARiTklAhGRmFMiEBGJuf8HI1xFh6UrLB0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rj913Lb6BGS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "784308fc-4b15-4aa3-cff9-61038471f8a1"
      },
      "source": [
        "acc = history_dict['acc']\n",
        "val_acc = history_dict['val_acc']\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9FYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNIU7KkdmmCD",
        "colab_type": "text"
      },
      "source": [
        "### Get test set performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTAkK_wwkOOQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "8e308582-dd7e-49cd-b7f8-b237a1e9ca48"
      },
      "source": [
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000/25000 [==============================] - 2s 92us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.2228203309708834, 0.84488]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaY2Z5o_mo8n",
        "colab_type": "text"
      },
      "source": [
        "### Transpose/reshape data so that it's shaped the way our manual implementation expects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVTtdrfK7dS4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = x_train.T\n",
        "y_train = y_train.reshape(1, y_train.shape[0])\n",
        "partial_x_train = partial_x_train.T\n",
        "partial_y_train = partial_y_train.reshape(1, partial_y_train.shape[0])\n",
        "x_val = x_val.T\n",
        "y_val = y_val.reshape(1, y_val.shape[0])\n",
        "x_test = x_test.T\n",
        "y_test = y_test.reshape(1, y_test.shape[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUPB3fyq9hfR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "d8252a00-f6c5-4ef8-ec28-dee8913450bb"
      },
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(partial_x_train.shape)\n",
        "print(partial_y_train.shape)\n",
        "print(x_val.shape)\n",
        "print(y_val.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 25000)\n",
            "(1, 25000)\n",
            "(10000, 15000)\n",
            "(1, 15000)\n",
            "(10000, 10000)\n",
            "(1, 10000)\n",
            "(10000, 25000)\n",
            "(1, 25000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chfeW2nwoexP",
        "colab_type": "text"
      },
      "source": [
        "### Time for you to do stuff!\n",
        "\n",
        "We're going to code this model up from scratch.  We will implement 5 functions:\n",
        "1. `initialize_params` will generate random starting values for the parameters b and w.\n",
        "2. `forward_prop` does the calculations for forward propagation.\n",
        "3. `backward_prop` does the calculations for backward propagation.\n",
        "4. `grad_check` does gradient checking so that we can be sure our backward propagation is correct.  I've written this one.\n",
        "5. `fit_model` does gradient descent to estimate the model parameters.\n",
        "\n",
        "Note that as defined above, our network has 3 layers:\n",
        " * Layer 1 has 16 units and a relu activation\n",
        " * Layer 2 has 16 units and a relu activation\n",
        " * Layer 3 has 1 unit and a sigmoid activation\n",
        "\n",
        "### 1. initialize_params\n",
        "\n",
        "Add the appropriate shape specifications to the initializations below.\n",
        "\n",
        "**Note that you are defining $w$, not $w^T$**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-dyx8N_8Mkr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_params(num_features, seed = 9433):\n",
        "  '''\n",
        "  Initialize parameter values for a network with 3 layers:\n",
        "  layer 1 has 16 units, layer 2 has 16 units, and layer 3 has 1 unit\n",
        "\n",
        "  Arguments:\n",
        "    - num_features: number of input features\n",
        "    - seed: seed to use for random number generation\n",
        "  \n",
        "  Return:\n",
        "    - Dictionary with initial values for b1, w1, b2, w2, b3, and w3\n",
        "  '''\n",
        "  # set seed\n",
        "  np.random.seed(seed)\n",
        "\n",
        "  # layer 1 parameters -- replace Nones with appropriate shape\n",
        "  b1 = np.random.standard_normal((16, 1)) * 0.1 \n",
        "  w1 = np.random.standard_normal((x_train.shape[0], 16)) * 0.1\n",
        "\n",
        "  # layer 2 parameters -- replace Nones with appropriate shape\n",
        "  b2 = np.random.standard_normal((16, 1)) * 0.1\n",
        "  w2 = np.random.standard_normal((16, 16)) * 0.1\n",
        "\n",
        "  # layer 3 parameters -- replace Nones with appropriate shape\n",
        "  b3 = np.random.standard_normal((1, 1)) * 0.1\n",
        "  w3 = np.random.standard_normal((16, 1)) * 0.1\n",
        "\n",
        "  return({\n",
        "      'b1': b1,\n",
        "      'w1': w1,\n",
        "      'b2': b2,\n",
        "      'w2': w2,\n",
        "      'b3': b3,\n",
        "      'w3': w3\n",
        "  })"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffbcK7QZrkiq",
        "colab_type": "text"
      },
      "source": [
        "### 2. forward_prop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkIvAcc7-DRV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward_prop(params, x):\n",
        "  '''\n",
        "  Forward propagation calculations\n",
        "\n",
        "  Arguments:\n",
        "    - params: dictionary with values for b1, w1, b2, w2, b3, w3\n",
        "    - x: matrix of features, shape (p, m)\n",
        "  \n",
        "  Return:\n",
        "    - Dictionary with z and a values for each layer\n",
        "  '''\n",
        "  # Pull out parameters from params dictionary\n",
        "  b1 = params['b1']\n",
        "  w1 = params['w1']\n",
        "  b2 = params['b2']\n",
        "  w2 = params['w2']\n",
        "  b3 = params['b3']\n",
        "  w3 = params['w3']\n",
        "  \n",
        "  # Calculate forward propagation\n",
        "  # You can use np.maximum(0, ___) to compute the relu activation function\n",
        "  # We have imported the sigmoid function\n",
        "  z1 = b1 + np.dot(w1.T, x)\n",
        "  a1 = np.maximum(0, z1)\n",
        "\n",
        "  z2 = b2 + np.dot(w2.T, a1)\n",
        "  a2 = np.maximum(0, z2)\n",
        "\n",
        "  z3 = b3 + np.dot(w3.T, a2)\n",
        "  a3 = sigmoid(z3)\n",
        "  \n",
        "  # Return dictionary of results from forward propagation\n",
        "  return({\n",
        "    'z1': z1,\n",
        "    'a1': a1,\n",
        "    'z2': z2,\n",
        "    'a2': a2,\n",
        "    'z3': z3,\n",
        "    'a3': a3\n",
        "  })"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-BSu3vhtG6J",
        "colab_type": "text"
      },
      "source": [
        "### 3. backward_prop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLIPXARSALF3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def backward_prop(params, x, y, forward_cache):\n",
        "  '''\n",
        "  Backward propagation calculations\n",
        "\n",
        "  Arguments:\n",
        "    - params: dictionary with values for b1, w1, b2, w2, b3, w3\n",
        "    - x: array of features, shape (p, m)\n",
        "    - y: array of responses, shape (1, m)\n",
        "    - forward_cache: Dictionary with z and a values for each layer\n",
        "      (return value from forward_prop)\n",
        "  \n",
        "  Return:\n",
        "    - Dictionary of derivatives of cost function J with respect to\n",
        "      b1, w1, b2, w2, b3, w3\n",
        "  '''\n",
        "  # Extract quantities needed for backward propagation calculations\n",
        "  m = x.shape[1]\n",
        "\n",
        "  z1 = forward_cache['z1']\n",
        "  a1 = forward_cache['a1']\n",
        "  z2 = forward_cache['z2']\n",
        "  a2 = forward_cache['a2']\n",
        "  z3 = forward_cache['z3']\n",
        "  a3 = forward_cache['a3']\n",
        "\n",
        "  w1 = params['w1']\n",
        "  w2 = params['w2']\n",
        "  w3 = params['w3']\n",
        "\n",
        "  # Backward propagation calculations\n",
        "  # Layer 3\n",
        "  dJdz3 = a3 - y\n",
        "  dJdb3 = np.mean(dJdz3, axis = 1, keepdims=True)\n",
        "  dJdw3 = (1/m) * np.dot(a2, dJdz3.T)\n",
        "\n",
        "  # Layer 2\n",
        "  dJda2 = np.dot(w3, dJdz3)\n",
        "  dJdz2 = dJda2 * (z2 >= 0).astype(float)\n",
        "  dJdb2 = np.mean(dJdz2, axis = 1, keepdims=True)\n",
        "  dJdw2 = (1/m) * np.dot(a1, dJdz2.T)\n",
        "\n",
        "  # Layer 1\n",
        "  dJda1 = np.dot(w2, dJdz2)\n",
        "  dJdz1 = dJda1 * (z1 >= 0).astype(float)\n",
        "  dJdb1 = np.mean(dJdz1, axis = 1, keepdims=True)\n",
        "  dJdw1 = (1/m) * np.dot(x, dJdz1.T)\n",
        "\n",
        "  return({\n",
        "      'dJdb1': dJdb1,\n",
        "      'dJdw1': dJdw1,\n",
        "      'dJdb2': dJdb2,\n",
        "      'dJdw2': dJdw2,\n",
        "      'dJdb3': dJdb3,\n",
        "      'dJdw3': dJdw3\n",
        "  })"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_tep-avtLXd",
        "colab_type": "text"
      },
      "source": [
        "### 4. grad_check\n",
        "I have implemented this in the two functions below; you just need to run the code.  After the function definitions, there are calls to check the gradient for a bias parameter and a weight parameter in each of the 3 layers.  Your results should match to at least 6 or 7 decimal places.\n",
        "\n",
        "Although I have implemented this, you should know how this works!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCvqYgt1FfB5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "73640c15-9ffe-4d33-cf4e-dcfefb50b1d3"
      },
      "source": [
        "def loss(a3, y):\n",
        "  '''\n",
        "  Calculate the binary crossentropy loss\n",
        "\n",
        "  Arguments:\n",
        "    - a3: output from layer 3\n",
        "    - y: observed responses\n",
        "  \n",
        "  Return:\n",
        "    - binary cross entropy\n",
        "  '''\n",
        "  m = y.shape[1]\n",
        "  inds_0 = np.where(y == 0)\n",
        "  inds_1 = np.where(y == 1)\n",
        "\n",
        "  return(-1 * (np.sum(np.log(a3[inds_1])) + np.sum(np.log(1 - a3[inds_0]))) / m)\n",
        "\n",
        "def grad_check(param_name, ind1, ind2, eps = 0.000001):\n",
        "  '''\n",
        "  Calculate gradient check by finite differencing\n",
        "\n",
        "  Arguments:\n",
        "    - param_name: name of parameter to check, e.g. 'w1'\n",
        "    - ind1, ind2: indices to check for the specified parameter\n",
        "    - eps: amount to add and subtract from the given parameter value\n",
        "  \n",
        "  Returns:\n",
        "    - No return value.  Prints diagnostic messages.\n",
        "  '''\n",
        "  params = initialize_params(num_features = x_train.shape[0])\n",
        "  forward_cache = forward_prop(params, x_train)\n",
        "\n",
        "  params[param_name][ind1, ind2] = params[param_name][ind1, ind2] + eps\n",
        "  forward_cache_plus_eps = forward_prop(params, x_train)\n",
        "  loss_plus_eps = loss(forward_cache_plus_eps['a3'], y_train)\n",
        "\n",
        "  params[param_name][ind1, ind2] = params[param_name][ind1, ind2] - 2*eps\n",
        "  forward_cache_minus_eps = forward_prop(params, x_train)\n",
        "  loss_minus_eps = loss(forward_cache_minus_eps['a3'], y_train)\n",
        "\n",
        "  est_grad = (loss_plus_eps - loss_minus_eps) / (2 * eps)\n",
        "  calc_grad = backward_prop(params, x_train, y_train, forward_cache)\n",
        "\n",
        "  print(\"\\ncheck of dJd\" + param_name + \"[\" + str(ind1) + \", \" + str(ind2) + \"]\")\n",
        "  print(\"estimated derivative = \" + str(est_grad))\n",
        "  print(\"calculated derivative = \" + str(calc_grad['dJd' + param_name][ind1, ind2]))\n",
        "\n",
        "\n",
        "grad_check('b3', 0, 0)\n",
        "grad_check('w3', 1, 0)\n",
        "grad_check('b2', 0, 0)\n",
        "grad_check('w2', 1, 0)\n",
        "grad_check('b1', 0, 0)\n",
        "grad_check('w1', 1, 0)\n"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "check of dJdb3[0, 0]\n",
            "estimated derivative = -0.06645035599550653\n",
            "calculated derivative = -0.06645035592818394\n",
            "\n",
            "check of dJdw3[1, 0]\n",
            "estimated derivative = -0.01705180768807324\n",
            "calculated derivative = -0.01705180760554546\n",
            "\n",
            "check of dJdb2[0, 0]\n",
            "estimated derivative = 4.741124159934884e-05\n",
            "calculated derivative = 4.7411200549146216e-05\n",
            "\n",
            "check of dJdw2[1, 0]\n",
            "estimated derivative = -2.0967116931558394e-06\n",
            "calculated derivative = -2.096640381475816e-06\n",
            "\n",
            "check of dJdb1[0, 0]\n",
            "estimated derivative = 0.002057349846040779\n",
            "calculated derivative = 0.002057349875226606\n",
            "\n",
            "check of dJdw1[1, 0]\n",
            "estimated derivative = 0.002057349846040779\n",
            "calculated derivative = 0.0020573498752266047\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAedPjUhvR_t",
        "colab_type": "text"
      },
      "source": [
        "### 5. fit_model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzQsOejUTLqX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit_model(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    num_epochs,\n",
        "    learning_rate,\n",
        "    initial_params):\n",
        "  '''\n",
        "  Fit our model by gradient descent.\n",
        "\n",
        "  Arguments:\n",
        "    - x_train: array of input features of shape (p, m)\n",
        "    - y_train: array of responses of shape (1, m)\n",
        "    - num_epochs: number of iterations of gradient descent to run\n",
        "    - learning_rate: learning rate for gradient descent\n",
        "    - initial_params: dictionary of starting parameter values\n",
        "  \n",
        "  Return:\n",
        "    - Dictionary of parameter estimates b1, w1, b2, w2, b3, w3\n",
        "  '''\n",
        "  # Get initial parameter values\n",
        "  params = initial_params\n",
        "\n",
        "  # For loop should iterate over the number of epochs\n",
        "  for i in range(num_epochs):\n",
        "    # print progress update\n",
        "    print(\"epoch \" + str(i))\n",
        "\n",
        "    # Do forward propagation\n",
        "    forward_cache = forward_prop(params, x_train)\n",
        "\n",
        "    # Calculate and print training set accuracy based on current parameter values\n",
        "    train_set_accuracy = np.mean(y_train == (forward_cache['a3'] >= 0.5).astype(float))\n",
        "    print(\"train set accuracy: \" + str(train_set_accuracy))\n",
        "\n",
        "    # Do backward propagation\n",
        "    calc_grad = backward_prop(params, x_train, y_train, forward_cache)\n",
        "\n",
        "    # Do parameter updates\n",
        "    # I would probably ordinarily loop over the parameter names to do this, but\n",
        "    # I think it will make more sense if you type things in by hand?\n",
        "    # Your update for b1 should involve params['b1'], calc_grad['dJdb1'], and\n",
        "    # the learning rate.\n",
        "    params['b1'] = params['b1'] - learning_rate * calc_grad['dJdb1']\n",
        "    params['w1'] = params['w1'] - learning_rate * calc_grad['dJdw1']\n",
        "    params['b2'] = params['b2'] - learning_rate * calc_grad['dJdb2']\n",
        "    params['w2'] = params['w2'] - learning_rate * calc_grad['dJdw2']\n",
        "    params['b3'] = params['b3'] - learning_rate * calc_grad['dJdb3']\n",
        "    params['w3'] = params['w3'] - learning_rate * calc_grad['dJdw3']\n",
        "  \n",
        "  # Return\n",
        "  return(params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SG--hlelw9eS",
        "colab_type": "text"
      },
      "source": [
        "Call the fit_model function.  To match what was done in the Keras code, you should use `partial_x_train` and `partial_y_train`.  To start, use 20 epochs and a learning rate of 0.1.\n",
        "\n",
        "This should take a little under a minute to run.  Your results won't be that impressive; I got up to about 59% classification accuracy on the training set after 20 epochs.  The reason we're not doing as well as Keras is that we are using a less fancy optimization algorithm.  We'll see on Friday how to make a small adjustment to our optimization routine to get much faster convergence.  For now, you could increase the number of training epochs if you want to."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4rHF2YQVFwG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720
        },
        "outputId": "71c84459-86f0-462f-8809-ef6e026325a5"
      },
      "source": [
        "# generate starting values for the parameters\n",
        "initial_params = initialize_params(num_features = partial_x_train.shape[0])\n",
        "\n",
        "# estimate the parameters by calling your fit_model function\n",
        "param_estimates = fit_model(\n",
        "  partial_x_train,\n",
        "  partial_y_train,\n",
        "  num_epochs=20,\n",
        "  learning_rate=0.5,\n",
        "  initial_params = initial_params)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0\n",
            "train set accuracy: 0.5035333333333334\n",
            "epoch 1\n",
            "train set accuracy: 0.5039333333333333\n",
            "epoch 2\n",
            "train set accuracy: 0.5062666666666666\n",
            "epoch 3\n",
            "train set accuracy: 0.5177333333333334\n",
            "epoch 4\n",
            "train set accuracy: 0.531\n",
            "epoch 5\n",
            "train set accuracy: 0.5392\n",
            "epoch 6\n",
            "train set accuracy: 0.5456\n",
            "epoch 7\n",
            "train set accuracy: 0.5500666666666667\n",
            "epoch 8\n",
            "train set accuracy: 0.5569333333333333\n",
            "epoch 9\n",
            "train set accuracy: 0.5603333333333333\n",
            "epoch 10\n",
            "train set accuracy: 0.5612666666666667\n",
            "epoch 11\n",
            "train set accuracy: 0.5640666666666667\n",
            "epoch 12\n",
            "train set accuracy: 0.5672666666666667\n",
            "epoch 13\n",
            "train set accuracy: 0.5706666666666667\n",
            "epoch 14\n",
            "train set accuracy: 0.5734666666666667\n",
            "epoch 15\n",
            "train set accuracy: 0.5761333333333334\n",
            "epoch 16\n",
            "train set accuracy: 0.5792\n",
            "epoch 17\n",
            "train set accuracy: 0.5812666666666667\n",
            "epoch 18\n",
            "train set accuracy: 0.5845333333333333\n",
            "epoch 19\n",
            "train set accuracy: 0.5874666666666667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xtph9P2y4Lw",
        "colab_type": "text"
      },
      "source": [
        "### Get test set performance based on your parameter estimates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Khm37iouVd2f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "47c71e19-bfc0-44fa-88c8-d3992313d9db"
      },
      "source": [
        "test_forward = forward_prop(param_estimates, x_test)\n",
        "y_test_hat = (test_forward['a3'] >= 0.5).astype(float)\n",
        "np.mean(y_test_hat == y_test)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.58624"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qb7sSkKuz0xo",
        "colab_type": "text"
      },
      "source": [
        "### Estimation by Stochastic Gradient Descent\n",
        "\n",
        "**We will talk about this on Friday!  Do not do this on Wednesday.**\n",
        "\n",
        "Implement a function to do stochastic gradient descent:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8o2RhfxEaE9e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit_model_SGD(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    num_epochs,\n",
        "    minibatch_size,\n",
        "    learning_rate,\n",
        "    initial_params):\n",
        "  '''\n",
        "  Estimate model parameters by stochastic gradient descent (SGD)\n",
        "\n",
        "  Arguments:\n",
        "    - x_train: array of input features of shape (p, m)\n",
        "    - y_train: array of responses of shape (1, m)\n",
        "    - num_epochs: number of iterations of gradient descent to run\n",
        "    - minibatch_size: number of observations in each minibatch\n",
        "    - learning_rate: learning rate for gradient descent\n",
        "    - initial_params: dictionary of starting parameter values\n",
        "  \n",
        "  Return:\n",
        "    - Dictionary of parameter estimates b1, w1, b2, w2, b3, w3\n",
        "  '''\n",
        "  params = initial_params\n",
        "\n",
        "  # calculate size of each minibatch\n",
        "  m = x_train.shape[1]\n",
        "  num_minibatches = m // minibatch_size + 1\n",
        "\n",
        "  # for loop over the number of epochs\n",
        "  for i in range(num_epochs):\n",
        "    print(\"epoch \" + str(i))\n",
        "    # find and print training set accuracy.  Kindof a waste of time, but nice to see.\n",
        "    forward_cache = forward_prop(params, x_train)\n",
        "    train_set_accuracy = np.mean(y_train == (forward_cache['a3'] >= 0.5).astype(float))\n",
        "    print(\"train set accuracy: \" + str(train_set_accuracy))\n",
        "\n",
        "    # for loop over minibatches within the current epoch\n",
        "    for j in range(num_minibatches):\n",
        "      # print progress indicator.  end = \"\" makes it so there is not a new line\n",
        "      # after each print statement\n",
        "      print(\".\", end = \"\")\n",
        "\n",
        "      # Set up a slice for the observation indices in the current minibatch\n",
        "      # Code to do this will be different depending on whether you're in the\n",
        "      # last minibatch or not.\n",
        "      if j == num_minibatches - 1:\n",
        "        minibatch_inds = slice(j*minibatch_size, m)\n",
        "      else:\n",
        "        minibatch_inds = slice(j*minibatch_size, (j+1)*minibatch_size)\n",
        "      \n",
        "      # pull out the x and y observations for this minibatch\n",
        "      x_train_minibatch = x_train[:, minibatch_inds]\n",
        "      y_train_minibatch = y_train[:, minibatch_inds]\n",
        "\n",
        "      # forward propagation based on this minibatch\n",
        "      forward_cache = forward_prop(params, x_train_minibatch)\n",
        "\n",
        "      # backward propagation based on this minibatch\n",
        "      calc_grad = backward_prop(params, x_train_minibatch, y_train_minibatch, forward_cache)\n",
        "\n",
        "      # gradient descent updates based on thsi minibatch\n",
        "      params['b1'] = params['b1'] - learning_rate * calc_grad['dJdb1']\n",
        "      params['w1'] = params['w1'] - learning_rate * calc_grad['dJdw1']\n",
        "      params['b2'] = params['b2'] - learning_rate * calc_grad['dJdb2']\n",
        "      params['w2'] = params['w2'] - learning_rate * calc_grad['dJdw2']\n",
        "      params['b3'] = params['b3'] - learning_rate * calc_grad['dJdb3']\n",
        "      params['w3'] = params['w3'] - learning_rate * calc_grad['dJdw3']\n",
        "  \n",
        "  # return parameters estimates\n",
        "  return(params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqtntev81T1t",
        "colab_type": "text"
      },
      "source": [
        "Now you can call the `fit_model_SGD` function to do stochastic gradient descent for our data set using 20 epochs, a minibatch size of 512 observations, and a learning rate of 0.1.  You should see much faster convergence!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTiuDwMCdDO8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 737
        },
        "outputId": "7a7e94e8-6b9a-41cd-8138-b9ca39df2c8a"
      },
      "source": [
        "# generate starting values for the parameters\n",
        "initial_params = initialize_params(num_features = partial_x_train.shape[0])\n",
        "\n",
        "# do estimation\n",
        "params_fit_mb = fit_model_SGD(\n",
        "  partial_x_train,\n",
        "  partial_y_train,\n",
        "  num_epochs=20,\n",
        "  minibatch_size=512,\n",
        "  learning_rate=0.1,\n",
        "  initial_params = initial_params)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0\n",
            "train set accuracy: 0.5035333333333334\n",
            "..............................epoch 1\n",
            "train set accuracy: 0.5432666666666667\n",
            "..............................epoch 2\n",
            "train set accuracy: 0.5676666666666667\n",
            "..............................epoch 3\n",
            "train set accuracy: 0.5846666666666667\n",
            "..............................epoch 4\n",
            "train set accuracy: 0.6043333333333333\n",
            "..............................epoch 5\n",
            "train set accuracy: 0.6289333333333333\n",
            "..............................epoch 6\n",
            "train set accuracy: 0.6589333333333334\n",
            "..............................epoch 7\n",
            "train set accuracy: 0.6868\n",
            "..............................epoch 8\n",
            "train set accuracy: 0.7167333333333333\n",
            "..............................epoch 9\n",
            "train set accuracy: 0.7416666666666667\n",
            "..............................epoch 10\n",
            "train set accuracy: 0.765\n",
            "..............................epoch 11\n",
            "train set accuracy: 0.7796666666666666\n",
            "..............................epoch 12\n",
            "train set accuracy: 0.7941333333333334\n",
            "..............................epoch 13\n",
            "train set accuracy: 0.8057333333333333\n",
            "..............................epoch 14\n",
            "train set accuracy: 0.8142666666666667\n",
            "..............................epoch 15\n",
            "train set accuracy: 0.8222\n",
            "..............................epoch 16\n",
            "train set accuracy: 0.832\n",
            "..............................epoch 17\n",
            "train set accuracy: 0.8384\n",
            "..............................epoch 18\n",
            "train set accuracy: 0.8448666666666667\n",
            "..............................epoch 19\n",
            "train set accuracy: 0.8511333333333333\n",
            ".............................."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5-YGjBQjzc6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1ebcee7c-8281-407e-988c-5b9e798e5f64"
      },
      "source": [
        "test_forward = forward_prop(params_fit_mb, x_test)\n",
        "y_test_hat = (test_forward['a3'] >= 0.5).astype(float)\n",
        "np.mean(y_test_hat == y_test)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8262"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iy10sabB3ZOe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}